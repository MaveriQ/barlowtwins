{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc0081ab-19a1-4a15-90c7-f5f5ac98b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerBase, BertTokenizer, BatchEncoding, BertModel, BertConfig\n",
    "import pdb\n",
    "from copy import deepcopy\n",
    "from datasets import load_dataset\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7507452d-d8e9-4236-a83b-50c5b834c91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset bookcorpus (/mounts/data/corp/huggingface/datasets/bookcorpus/plain_text/1.0.0/44662c4a114441c35200992bea923b170e6f13f2f0beb7c14e43759cec498700)\n"
     ]
    }
   ],
   "source": [
    "corpus = load_dataset('bookcorpus',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "631d47b9-46a0-4866-a199-361293a5afcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 74004228\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cfef20-3153-435c-a878-ad689507fb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the half-ling book one in the fall of igneeria series kaylee soderburg copyright 2013 kaylee soderburg all rights reserved .'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f732ed85-e4bc-4eca-9755-0e032887c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForBarlowBertWithMLM:\n",
    "    \"\"\"\n",
    "    Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they\n",
    "    are not all of the same length.\n",
    "    Args:\n",
    "        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n",
    "            The tokenizer used for encoding the data.\n",
    "        mlm (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Whether or not to use masked language modeling. If set to :obj:`False`, the labels are the same as the\n",
    "            inputs with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for\n",
    "            non-masked tokens and the value to predict for the masked token.\n",
    "        mlm_probability (:obj:`float`, `optional`, defaults to 0.15):\n",
    "            The probability with which to (randomly) mask tokens in the input, when :obj:`mlm` is set to :obj:`True`.\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "    .. note::\n",
    "        For best performance, this data collator should be used with a dataset having items that are dictionaries or\n",
    "        BatchEncoding, with the :obj:`\"special_tokens_mask\"` key, as returned by a\n",
    "        :class:`~transformers.PreTrainedTokenizer` or a :class:`~transformers.PreTrainedTokenizerFast` with the\n",
    "        argument :obj:`return_special_tokens_mask=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.mlm and self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. \"\n",
    "                \"You should pass `mlm=False` to train on causal language modeling instead.\"\n",
    "            )\n",
    "\n",
    "    def __call__(\n",
    "        self, examples: List[Union[List[int], torch.Tensor, Dict[str, torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], (dict, BatchEncoding)):\n",
    "            batch = self.tokenizer.pad(examples, \n",
    "                                        return_tensors=\"pt\", \n",
    "                                        pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            print('Error. This collator only works with dicts or BatchEncoded inputs')\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        batch_1 = deepcopy(batch)\n",
    "        batches = []\n",
    "\n",
    "        batch[\"mlm_input_ids\"],batch[\"mlm_labels\"] = self.mask_tokens(batch[\"input_ids\"], special_tokens_mask=special_tokens_mask)\n",
    "        batches.append(batch)\n",
    "        batch_1[\"mlm_input_ids\"],batch_1[\"mlm_labels\"] = self.mask_tokens(batch_1[\"input_ids\"], special_tokens_mask=special_tokens_mask)\n",
    "        batches.append(batch_1)\n",
    "        \n",
    "        # batches.append({'masked_indices': masked_indices_1 | masked_indices_2})\n",
    "            \n",
    "        return batches\n",
    "\n",
    "    def mask_tokens(\n",
    "        self, inputs: torch.Tensor, special_tokens_mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        labels = inputs.clone()\n",
    "        inputs = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(inputs.shape, self.mlm_probability)\n",
    "#         if special_tokens_mask is None:\n",
    "#             special_tokens_mask = [\n",
    "#                 self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in inputs.tolist()\n",
    "#             ]\n",
    "#             special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "#         else:\n",
    "        special_tokens_mask = special_tokens_mask.bool()\n",
    "#         pdb.set_trace()\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        inputs[masked_indices] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # # The rest of the time (20% of the time) we keep the masked input tokens unchanged\n",
    "        \n",
    "        #  # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        #  # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "307c046d-8848-4bf1-ba22-8af8ae795fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')#, padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2e97bb4-8ed2-4e9f-9bb0-b515e39f548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForBarlowBertWithMLM(tokenizer,mlm_probability=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab60ce7d-ce09-4b10-a4c0-aa58b6cb3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [tokenizer(corpus[i]['text'],return_tensors=\"pt\",padding='max_length',truncation=True,max_length=32,return_special_tokens_mask=True,return_token_type_ids=False) for i in range(4) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0496d02e-81a0-480e-9655-0c14b17fdcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[  101,  1996,  2431,  1011, 17002,  2338,  2028,  1999,  1996,  2991,\n",
       "           1997, 16270, 11510,  2401,  2186, 10905, 10559,  2061,  4063,  4645,\n",
       "           9385,  2286, 10905, 10559,  2061,  4063,  4645,  2035,  2916,  9235,\n",
       "           1012,   102]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[  101,  3175,  1024, 17332, 24594, 17134,  2581, 21486,  3175,  1011,\n",
       "           2410,  1024,  4891,  1011, 17332, 24594, 17134,  2581, 22394,  2005,\n",
       "           2026,  2155,  1010,  2040,  6628,  2033,  2000,  2196,  2644,  3554,\n",
       "           2005,   102]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[ 101, 1045, 4299, 1045, 2018, 1037, 2488, 3437, 2000, 2008, 3160, 1012,\n",
       "           102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  2732, 11227,  1010,  2047,  2259,  2003,  2025,  1996,  2173,\n",
       "           2017,  2094,  5987,  2172,  2000,  4148,  1012,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e398e97-f517-4888-898b-d42a12f8cc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] the half - ling book one in the fall of igneeria series kaylee soderburg copyright 2013 kaylee soderburg all rights reserved. [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs[0]['input_ids'][0].squeeze().data.numpy(),skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "750dc749-d43d-41da-8e06-f10f0b3dc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = collator(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20a2e00b-1e80-4282-adbf-568bddc2ff2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "722496dc-028b-471c-988e-57c4d032ad80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  101,  1996,  2431,  1011, 17002,  2338,  2028,  1999,  1996,  2991,\n",
       "           1997, 16270, 11510,  2401,  2186, 10905, 10559,  2061,  4063,  4645,\n",
       "           9385,  2286, 10905, 10559,  2061,  4063,  4645,  2035,  2916,  9235,\n",
       "           1012,   102]],\n",
       "\n",
       "        [[  101,  3175,  1024, 17332, 24594, 17134,  2581, 21486,  3175,  1011,\n",
       "           2410,  1024,  4891,  1011, 17332, 24594, 17134,  2581, 22394,  2005,\n",
       "           2026,  2155,  1010,  2040,  6628,  2033,  2000,  2196,  2644,  3554,\n",
       "           2005,   102]],\n",
       "\n",
       "        [[  101,  1045,  4299,  1045,  2018,  1037,  2488,  3437,  2000,  2008,\n",
       "           3160,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]],\n",
       "\n",
       "        [[  101,  2732, 11227,  1010,  2047,  2259,  2003,  2025,  1996,  2173,\n",
       "           2017,  2094,  5987,  2172,  2000,  4148,  1012,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a06a30d5-b59b-4236-aef2-2db5b21cebc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  101,   103,   103,   103, 17002,   103,   103,   103,   103,   103,\n",
       "            103,  4957,   103,  4856,  2186,   103,   103,   103,   103,   103,\n",
       "            103,   103,   103, 25434,   103, 20353,   103,   103,   103,   103,\n",
       "           1012,   102]],\n",
       "\n",
       "        [[  101,   103,   103,   103,   103, 17134,   103,   103,  3175,   392,\n",
       "            103,   103,   103,  1011,   103,   103,   103,   103,   103,  2005,\n",
       "           1191,   103,   103, 28837,   103,   103,   103,   103,   103, 19245,\n",
       "            103,   102]],\n",
       "\n",
       "        [[  101,   103,   103,   103,   103,   103,   103,  3437,   103,   103,\n",
       "            103,   103,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]],\n",
       "\n",
       "        [[  101,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
       "            103,  2094,   103,   103,   103,   103,   103,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]['mlm_input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8caa5788-c2a6-4ad2-8bda-9b1ee60f62ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] the half - ling book one in the fall of igneeria series kaylee soderburg copyright 2013 kaylee soderburg all rights reserved. [SEP]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0]['input_ids'][0].squeeze().data.numpy(),skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8231c80f-ee3c-4a63-8e95-613442dcc78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [MASK] [MASK] [MASK] ling [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] link [MASK] 1950s series [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]national [MASK] biotechnology [MASK] [MASK] [MASK] [MASK]. [SEP]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0]['mlm_input_ids'][0].squeeze().data.numpy(),skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8440c6cf-16e1-4039-95f5-0bf25220378a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]っ [MASK]gra [MASK] [MASK] [MASK] ready ethanol [MASK] [MASK] so [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [SEP]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[1]['mlm_input_ids'][0].squeeze().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f9fccd8-56e7-4eff-ae35-338a09055310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9375)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out[0]['input_ids'][0]==out[0]['mlm_input_ids'][0]).sum()/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7017f23d-657a-42ae-aee4-0b22113c0251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0312), tensor(0.1875))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out[0]['mlm_input_ids'][0]==103).sum()/32, (out[1]['mlm_input_ids'][0]==103).sum()/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c476ad61-2d64-4b9d-8e2a-3889eb6744e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0625)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out[0]['mlm_labels'][0]!=-100).sum()/(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ddc20-aca2-4978-8d4a-045644dabc27",
   "metadata": {},
   "source": [
    "## Dropout BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "38207d12-6d71-4572-899f-44c0aae93d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1a1b0685-cb11-42bf-8e3f-9f997158cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tiny = {\n",
    "    \"hidden_size\" : 128 ,\n",
    "    \"num_hidden_layers\" : 2,\n",
    "    \"num_attention_heads\": int(128/64),\n",
    "    \"intermediate_size\" : int(128*4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "20434e20-ae61-42ee-be12-b6a9465acf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dropout = {\n",
    "    \"attention_probs_dropout_prob\": 0.0,\n",
    "    \"hidden_dropout_prob\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "27001d22-53b7-4aa1-af61-dcaba6dd8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(bert_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ef0da66d-095b-4f1c-9def-e32ffeb8681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(no_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "416f500c-6225-4563-8bba-10abff180d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.8.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "98f01dca-3da1-4448-a9f5-5cea7d3422b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel(config, add_pooling_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b48f2086-2d37-40a5-95ae-161dbf20b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [tokenizer(corpus[i]['text'],return_tensors=\"pt\",padding='max_length',truncation=True,max_length=32,return_special_tokens_mask=False,return_token_type_ids=True) for i in range(4) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "264db71d-9b5d-433f-af14-d5e858ee03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = bert_model(**inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a449a339-87c8-4a63-b77d-87285796c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = bert_model(**inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a630ace5-7eae-4134-8a38-5b13c513c429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2360,  0.3188,  0.0112,  0.2215,  0.2356, -0.0209,  0.0389, -0.1634,\n",
       "          0.0339,  0.2450,  0.0752, -0.3388,  0.2422, -0.3530, -0.1269,  0.1579,\n",
       "         -0.1193,  0.1439, -0.0313, -0.0605,  0.0805,  0.1103,  0.3938,  0.1160,\n",
       "          0.3036, -0.2942,  0.2283,  0.1398,  0.0811,  0.1837, -0.0860, -0.1693,\n",
       "         -0.4958,  0.1911,  0.3638, -0.3315, -0.0873, -0.1776, -0.1431, -0.2119,\n",
       "          0.3633, -0.0500,  0.0549, -0.0465, -0.0073, -0.0145, -0.1697, -0.1467,\n",
       "         -0.0921,  0.0206, -0.0330, -0.1368, -0.0939, -0.0871, -0.1688, -0.1531,\n",
       "         -0.1137,  0.0058, -0.1541, -0.3338, -0.0227, -0.0152,  0.0033, -0.0960,\n",
       "          0.0118, -0.2984,  0.0507,  0.3106, -0.2967,  0.2001, -0.0014, -0.1241,\n",
       "         -0.1981, -0.0143, -0.1187,  0.2613, -0.1896,  0.1479, -0.2526, -0.3897,\n",
       "          0.1129,  0.3664, -0.3909, -0.2956, -0.3465, -0.2942, -0.0788,  0.0301,\n",
       "          0.0533, -0.2214, -0.4570,  0.1595, -0.2155, -0.0009,  0.3324,  0.1567,\n",
       "          0.0501, -0.0744,  0.0441, -0.0211,  0.0281, -0.0138, -0.2687,  0.2718,\n",
       "          0.0399,  0.0728, -0.2143,  0.2367, -0.1553, -0.0304, -0.0037, -0.2661,\n",
       "          0.2228, -0.0854,  0.2266,  0.1094,  0.0686, -0.1138, -0.1416, -0.2151,\n",
       "         -0.3364,  0.2192, -0.1098, -0.3573,  0.5024,  0.3139, -0.2853,  0.1062]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "88c603ca-1f44-425b-8b24-e74d080b705a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2360,  0.3188,  0.0112,  0.2215,  0.2356, -0.0209,  0.0389, -0.1634,\n",
       "          0.0339,  0.2450,  0.0752, -0.3388,  0.2422, -0.3530, -0.1269,  0.1579,\n",
       "         -0.1193,  0.1439, -0.0313, -0.0605,  0.0805,  0.1103,  0.3938,  0.1160,\n",
       "          0.3036, -0.2942,  0.2283,  0.1398,  0.0811,  0.1837, -0.0860, -0.1693,\n",
       "         -0.4958,  0.1911,  0.3638, -0.3315, -0.0873, -0.1776, -0.1431, -0.2119,\n",
       "          0.3633, -0.0500,  0.0549, -0.0465, -0.0073, -0.0145, -0.1697, -0.1467,\n",
       "         -0.0921,  0.0206, -0.0330, -0.1368, -0.0939, -0.0871, -0.1688, -0.1531,\n",
       "         -0.1137,  0.0058, -0.1541, -0.3338, -0.0227, -0.0152,  0.0033, -0.0960,\n",
       "          0.0118, -0.2984,  0.0507,  0.3106, -0.2967,  0.2001, -0.0014, -0.1241,\n",
       "         -0.1981, -0.0143, -0.1187,  0.2613, -0.1896,  0.1479, -0.2526, -0.3897,\n",
       "          0.1129,  0.3664, -0.3909, -0.2956, -0.3465, -0.2942, -0.0788,  0.0301,\n",
       "          0.0533, -0.2214, -0.4570,  0.1595, -0.2155, -0.0009,  0.3324,  0.1567,\n",
       "          0.0501, -0.0744,  0.0441, -0.0211,  0.0281, -0.0138, -0.2687,  0.2718,\n",
       "          0.0399,  0.0728, -0.2143,  0.2367, -0.1553, -0.0304, -0.0037, -0.2661,\n",
       "          0.2228, -0.0854,  0.2266,  0.1094,  0.0686, -0.1138, -0.1416, -0.2151,\n",
       "         -0.3364,  0.2192, -0.1098, -0.3573,  0.5024,  0.3139, -0.2853,  0.1062]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "78282a13-8631-4975-bb28-46c2def27342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2ade529f-f360-4741-8a4f-be0341dd070b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.pooler_output-out2.pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c4311-0233-4716-92f7-6173570b0dfa",
   "metadata": {},
   "source": [
    "### with dropout=0.0, we get same representation when passing a sentence twice. Hence the only difference in representation comes from dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3651b30-1ede-434f-aae1-19adb5eb38a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
