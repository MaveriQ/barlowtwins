{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c25769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CONFIG_MAPPING, BertForMaskedLM, BertModel, BertPreTrainedModel, BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "157badde-b45a-4614-b313-2497cf26aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_param = lambda model : sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9ed367",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT2FN = {\n",
    "    \"relu\": F.relu,\n",
    "    \"silu\": F.silu,\n",
    "    \"swish\": F.silu,\n",
    "    \"gelu\": F.gelu,\n",
    "    \"tanh\": torch.tanh,\n",
    "#     \"gelu_new\": gelu_new,\n",
    "#     \"gelu_fast\": gelu_fast,\n",
    "#     \"mish\": mish,\n",
    "#     \"linear\": linear_act,\n",
    "    \"sigmoid\": torch.sigmoid,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b45f46a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7761cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_small = {\n",
    "    \"hidden_size\" : 512 ,\n",
    "    \"num_hidden_layers\" : 4,\n",
    "    \"num_attention_heads\": int(512/64),\n",
    "    \"intermediate_size\" : int(512*4)\n",
    "}\n",
    "\n",
    "bert_tiny = {\n",
    "    \"hidden_size\" : 128 ,\n",
    "    \"num_hidden_layers\" : 2,\n",
    "    \"num_attention_heads\": int(128/64),\n",
    "    \"intermediate_size\" : int(128*4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa4599cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"projector\": \"256-256-256\",\n",
       "  \"transformers_version\": \"4.5.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = CONFIG_MAPPING['bert']()\n",
    "config.update(bert_tiny)\n",
    "config.projector='256-256-256'\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fbe0e37-c90c-404b-8093-f886d1f0203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel(config=config)\n",
    "bert_lm = BertForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "765aa2cd-dd62-47c8-9492-dd8b87b82263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2572, 2183, 2000, 4068, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('i am going to berlin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab5800e-4bb3-46df-9638-7f8b971fa32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i am going to berlin [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer('i am going to berlin')['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e376bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    def __init__(self, config,input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_size, output_size,bias=False)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(output_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0efe923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForBarlowTwins(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "#         pdb.set_trace()\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if config.is_decoder:\n",
    "            logger.warning(\n",
    "                \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n",
    "                \"bi-directional self-attention.\"\n",
    "            )\n",
    "            \n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        sizes = [config.hidden_size] + list(map(int, config.projector.split('-')))\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(Projector(config,sizes[i], sizes[i + 1]))\n",
    "        layers.append(Projector(config,sizes[-2], sizes[-1]))\n",
    "        self.projector = nn.Sequential(*layers)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "#     def get_output_embeddings(self):\n",
    "#         return self.cls.decoder\n",
    "\n",
    "#     def set_output_embeddings(self, new_embeddings):\n",
    "#         self.cls.decoder = new_embeddings\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        return self.projector(outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f696483",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = Projector(config,128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f919fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertForBarlowTwins(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58d93e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForBarlowTwins(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): Sequential(\n",
       "    (0): Projector(\n",
       "      (dense): Linear(in_features=128, out_features=256, bias=False)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Projector(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Projector(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddf98a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\",padding='max_length',max_length=32)\n",
    "input_2 = tokenizer(\"I am going to Berlin.\", return_tensors=\"pt\",padding='max_length',max_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b46e0f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = bert(**input_1)\n",
    "output2 = bert(**input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e582d7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 32, 256]), torch.Size([1, 32, 256]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.shape, output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f3c599b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-74bbbba2546f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0moutput2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "output1.T @ output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d4752111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 9, 1])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42736a5f-145a-438c-8574-8ff7566b6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_lm = BertForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e625f46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1294, 0.2320, 0.1160, 0.2871, 0.2027, 0.1145, 0.0425, 0.1529, 0.2508],\n",
       "        [0.3112, 0.5580, 0.2789, 0.6905, 0.4875, 0.2755, 0.1022, 0.3677, 0.6032],\n",
       "        [0.0291, 0.0523, 0.0261, 0.0647, 0.0457, 0.0258, 0.0096, 0.0344, 0.0565],\n",
       "        [0.0956, 0.1714, 0.0857, 0.2122, 0.1498, 0.0847, 0.0314, 0.1130, 0.1853],\n",
       "        [0.1932, 0.3463, 0.1731, 0.4286, 0.3026, 0.1710, 0.0634, 0.2282, 0.3744],\n",
       "        [0.0050, 0.0090, 0.0045, 0.0111, 0.0079, 0.0044, 0.0016, 0.0059, 0.0097],\n",
       "        [0.0950, 0.1703, 0.0851, 0.2107, 0.1487, 0.0841, 0.0312, 0.1122, 0.1840],\n",
       "        [0.1233, 0.2211, 0.1105, 0.2737, 0.1932, 0.1092, 0.0405, 0.1457, 0.2391],\n",
       "        [0.0313, 0.0561, 0.0280, 0.0694, 0.0490, 0.0277, 0.0103, 0.0369, 0.0606]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(9,1)@torch.rand(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "375e9000",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel(config, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46089be7-2486-4e5d-b385-a08bcb18dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_output = bert_model(**input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7ccb8a0-56ba-484d-84fe-aa6e906df1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94d06bff-5e2b-43e1-b413-7b58b5694d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1996, 3007, 1997, 2605, 2003,  103, 1012,  102,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a7be4-e7f1-4ffd-a491-743c71f83541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
