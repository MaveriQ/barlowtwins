{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cfeba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CONFIG_MAPPING, BertForMaskedLM, BertModel, BertPreTrainedModel, BertTokenizer, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e231a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_param = lambda model : sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00cbc0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT2FN = {\n",
    "    \"relu\": F.relu,\n",
    "    \"silu\": F.silu,\n",
    "    \"swish\": F.silu,\n",
    "    \"gelu\": F.gelu,\n",
    "    \"tanh\": torch.tanh,\n",
    "#     \"gelu_new\": gelu_new,\n",
    "#     \"gelu_fast\": gelu_fast,\n",
    "#     \"mish\": mish,\n",
    "#     \"linear\": linear_act,\n",
    "    \"sigmoid\": torch.sigmoid,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d5d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "935dc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_small = {\n",
    "    \"hidden_size\" : 512 ,\n",
    "    \"num_hidden_layers\" : 4,\n",
    "    \"num_attention_heads\": int(512/64),\n",
    "    \"intermediate_size\" : int(512*4)\n",
    "}\n",
    "\n",
    "bert_tiny = {\n",
    "    \"hidden_size\" : 128 ,\n",
    "    \"num_hidden_layers\" : 2,\n",
    "    \"num_attention_heads\": int(128/64),\n",
    "    \"intermediate_size\" : int(128*4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d44370fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"projector\": \"256-256-256\",\n",
       "  \"transformers_version\": \"4.5.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = CONFIG_MAPPING['bert']()\n",
    "config.update(bert_tiny)\n",
    "config.projector='256-256-256'\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aae67121",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel(config=config)\n",
    "bert_lm = BertForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b066aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2572, 2183, 2000, 4068, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('i am going to berlin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86af0514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i am going to berlin [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer('i am going to berlin')['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbbbcc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    def __init__(self, config,input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_size, output_size,bias=False)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(output_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "382c1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForBarlowTwins(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "#         pdb.set_trace()\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if config.is_decoder:\n",
    "            logger.warning(\n",
    "                \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n",
    "                \"bi-directional self-attention.\"\n",
    "            )\n",
    "            \n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        sizes = [config.hidden_size] + list(map(int, config.projector.split('-')))\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(Projector(config,sizes[i], sizes[i + 1]))\n",
    "        layers.append(Projector(config,sizes[-2], sizes[-1]))\n",
    "        self.projector = nn.Sequential(*layers)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "#     def get_output_embeddings(self):\n",
    "#         return self.cls.decoder\n",
    "\n",
    "#     def set_output_embeddings(self, new_embeddings):\n",
    "#         self.cls.decoder = new_embeddings\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        return self.projector(outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30c35385",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = Projector(config,128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a4be1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertForBarlowTwins(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00cf4126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForBarlowTwins(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): Sequential(\n",
       "    (0): Projector(\n",
       "      (dense): Linear(in_features=128, out_features=256, bias=False)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Projector(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Projector(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7438b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\",padding='max_length',max_length=32)\n",
    "input_2 = tokenizer(\"I am going to Berlin.\", return_tensors=\"pt\",padding='max_length',max_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "117a3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = bert(**input_1)\n",
    "output2 = bert(**input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f565ec7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 32, 256]), torch.Size([1, 32, 256]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.shape, output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "93e19ade",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-74bbbba2546f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0moutput2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "output1.T @ output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d15e6ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 9, 1])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93df6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_lm = BertForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "476d2d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1294, 0.2320, 0.1160, 0.2871, 0.2027, 0.1145, 0.0425, 0.1529, 0.2508],\n",
       "        [0.3112, 0.5580, 0.2789, 0.6905, 0.4875, 0.2755, 0.1022, 0.3677, 0.6032],\n",
       "        [0.0291, 0.0523, 0.0261, 0.0647, 0.0457, 0.0258, 0.0096, 0.0344, 0.0565],\n",
       "        [0.0956, 0.1714, 0.0857, 0.2122, 0.1498, 0.0847, 0.0314, 0.1130, 0.1853],\n",
       "        [0.1932, 0.3463, 0.1731, 0.4286, 0.3026, 0.1710, 0.0634, 0.2282, 0.3744],\n",
       "        [0.0050, 0.0090, 0.0045, 0.0111, 0.0079, 0.0044, 0.0016, 0.0059, 0.0097],\n",
       "        [0.0950, 0.1703, 0.0851, 0.2107, 0.1487, 0.0841, 0.0312, 0.1122, 0.1840],\n",
       "        [0.1233, 0.2211, 0.1105, 0.2737, 0.1932, 0.1092, 0.0405, 0.1457, 0.2391],\n",
       "        [0.0313, 0.0561, 0.0280, 0.0694, 0.0490, 0.0277, 0.0103, 0.0369, 0.0606]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(9,1)@torch.rand(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c6f9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel(config, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53b1b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_output = bert_model(**input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cc1aec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a5a87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\",padding='max_length',max_length=32,return_special_tokens_mask=True,return_token_type_ids=False)\n",
    "input_2 = tokenizer(\"I am going to Berlin.\", return_tensors=\"pt\",padding='max_length',max_length=32,return_special_tokens_mask=True,return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "174c18fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1996, 3007, 1997, 2605, 2003, 3000, 1012,  102,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "014b2c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2572, 2183, 2000, 4068, 1012,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "875237d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForLanguageModeling(tokenizer,mlm_probability=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f6b3ec5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[ 101, 1996,  103, 1997,  103,  103,  103,  103,  102,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "\n",
       "        [[ 101, 1045, 4140,  103, 2000,  103, 1012,  102,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0]]]), 'labels': tensor([[[-100, -100, 3007, -100, 2605, 2003, 3000, 1012, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]],\n",
       "\n",
       "        [[-100, 1045, 2572, 2183, -100, 4068, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]]])}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = collator([input_1,input_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a9127350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1045, 4140,  103,  103])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp['input_ids'][1][inp['labels'][1]!=-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df5a71c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iot [MASK] [MASK]'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(_,skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06165eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] iot [MASK] to [MASK]. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inp['input_ids'][1][0],skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d26ce761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 1045, 4140,  103, 2000,  103, 1012,  102,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp['input_ids'][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6919c9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i am going to berlin. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_2['input_ids'][0],skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "72f86cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2572, 2183, 2000, 4068, 1012,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "145a44c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##ot'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.ids_to_tokens[4140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809102f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
