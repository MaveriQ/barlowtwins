{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103fc83f-a4f7-4d29-9a09-b6ab1772c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CONFIG_MAPPING, BertForMaskedLM, BertModel, BertPreTrainedModel, BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25c4aaf-4d6c-4ce2-bc90-e0e3ca4d71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT2FN = {\n",
    "    \"relu\": F.relu,\n",
    "    \"silu\": F.silu,\n",
    "    \"swish\": F.silu,\n",
    "    \"gelu\": F.gelu,\n",
    "    \"tanh\": torch.tanh,\n",
    "    \"sigmoid\": torch.sigmoid,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c2830f-c5b9-4baf-8ba2-78909499fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tiny = {\n",
    "    \"hidden_size\" : 128 ,\n",
    "    \"num_hidden_layers\" : 2,\n",
    "    \"num_attention_heads\": int(128/64),\n",
    "    \"intermediate_size\" : int(128*4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c277d19d-55c7-49c7-820c-9566e9235f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"projector\": \"128-128\",\n",
       "  \"transformers_version\": \"4.8.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = CONFIG_MAPPING['bert']()\n",
    "config.update(bert_tiny)\n",
    "config.projector='128-128'\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4efc5e-200a-4f44-86f3-dc5c84926ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    def __init__(self, config,input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(input_size, output_size,bias=False)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(output_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ebc783-2e4a-4c6d-b19c-142b26c1ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    # return a flattened view of the off-diagonal elements of a square matrix\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "569987d5-b617-4246-82ef-a1f4c83e476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForBarlowTwins(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "#         pdb.set_trace()\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "            \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if config.is_decoder:\n",
    "            logger.warning(\n",
    "                \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n",
    "                \"bi-directional self-attention.\"\n",
    "            )\n",
    "            \n",
    "        self.bert = BertModel(config, add_pooling_layer=True)\n",
    "        \n",
    "        sizes = [config.hidden_size] + list(map(int, config.projector.split('-')))\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(Projector(config,sizes[i], sizes[i + 1]))\n",
    "        layers.append(Projector(config,sizes[-2], sizes[-1]))\n",
    "        self.projector = nn.Sequential(*layers)\n",
    "        self.bn = nn.BatchNorm1d((128,128), affine=False)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "#     def get_output_embeddings(self):\n",
    "#         return self.cls.decoder\n",
    "\n",
    "#     def set_output_embeddings(self, new_embeddings):\n",
    "#         self.cls.decoder = new_embeddings\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "#         pdb.set_trace()\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        projection = self.projector(outputs.pooler_output)\n",
    "        \n",
    "        return projection#,self.bn(projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89f589e8-f2f5-40d1-b2e4-35b3fc039d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForBarlowTwins(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc80fbb1-1b70-4705-900c-5f152c3ecd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')#, padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36fb5efa-8866-4ded-b0b6-6477febc4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\",padding='max_length',truncation=True)\n",
    "input_2 = tokenizer(\"I am going to Berlin.\", return_tensors=\"pt\",padding='max_length',truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3701d049-d930-498d-a652-1352e87a831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_2=model(**input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25640339-446a-4500-bd3d-b1d3d724649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarlowBert(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = BertForBarlowTwins(config)\n",
    "        self.scale_loss = 1/32\n",
    "        self.lambd = 3.9e-3\n",
    "        \n",
    "    def forward(self, y1, y2):\n",
    "        output1 = self.model(**y1)\n",
    "        output2 = self.model(**y2)\n",
    "        \n",
    "        c = (output1.transpose(0,1) @ output2)\n",
    "        \n",
    "        # use --scale-loss to multiply the loss by a constant factor\n",
    "        # see the Issues section of the readme\n",
    "        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum().mul(self.scale_loss)\n",
    "        off_diag = off_diagonal(c).pow_(2).sum().mul(self.scale_loss)\n",
    "        loss = on_diag + self.lambd * off_diag\n",
    "        return loss      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8877c89f-804d-4d80-be3c-63089078671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=(out_2.T@out_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da1785f6-93ff-46a6-b3e5-5dfc28989a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(238.8449, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diagonal(c).add_(-1).pow_(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "254e8df8-e917-4e29-8a88-0cdc3a9362b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16017.1543, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_diagonal(c).pow_(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d448951-2aa9-4b9f-8478-a29056b6b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "barlow = BarlowBert(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3236d5d2-ed3e-42b5-a79d-89003b7bafbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.7527, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barlow(input_1,input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf480f-f2d5-4c29-b99d-bb1bee1d49c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
