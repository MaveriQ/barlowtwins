{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca6063b6-89b7-4aab-b7f4-ade2e251c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import Callback, LearningRateMonitor\n",
    "from collections import namedtuple\n",
    "import pdb\n",
    "from pytorch_lightning.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ab2c25-9316-4ae0-b65b-3287887b19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoringModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = torch.nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self(batch).sum()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self(batch).sum()\n",
    "        self.log(\"valid_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self(batch).sum()\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.layer.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcf992cb-d32e-4eb8-8f13-c0eb45decf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoringModelSWA(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args,model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.args = args\n",
    "#         self.model = BoringModel()\n",
    "        self.swa_model = AveragedModel(model)\n",
    "        self.swa_start_step = self.args.swa_start_step\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.swa_model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optim = torch.optim.AdamW(self.swa_model.module.parameters(), lr=self.args.lr,weight_decay=self.args.weight_decay)\n",
    "        print(f\"Total number of training steps : {self.num_training_steps}\")\n",
    "        sched = torch.optim.lr_scheduler.OneCycleLR(optim,max_lr=self.args.lr,total_steps=self.num_training_steps,anneal_strategy='linear')\n",
    "        self.swa_scheduler = SWALR(optim, swa_lr=self.args.lr)\n",
    "\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": sched,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "#             \"name\": 'lr_scheduler',\n",
    "        }\n",
    "        return {'optimizer':optim,\n",
    "                'scheduler':lr_scheduler_config\n",
    "                }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self(batch).sum()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        if self.trainer.global_step > self.swa_start_step:\n",
    "#             pdb.set_trace()\n",
    "            self.swa_model.update_parameters(self.swa_model.module)\n",
    "            self.swa_scheduler.step()\n",
    "            \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    @property\n",
    "    def num_training_steps(self) -> int:\n",
    "        \"\"\"Total training steps inferred from datamodule and devices.\"\"\"\n",
    "#         pdb.set_trace()\n",
    "        if self.trainer.max_steps:\n",
    "            return self.trainer.max_steps\n",
    "\n",
    "        limit_batches = self.trainer.limit_train_batches\n",
    "        batches = len(self.train_dataloader())\n",
    "        batches = min(batches, limit_batches) if isinstance(limit_batches, int) else int(limit_batches * batches)     \n",
    "\n",
    "        num_devices = max(1, self.trainer.num_gpus, self.trainer.num_processes)\n",
    "        if self.trainer.tpu_cores:\n",
    "            num_devices = max(num_devices, self.trainer.tpu_cores)\n",
    "\n",
    "        effective_accum = self.trainer.accumulate_grad_batches * num_devices\n",
    "        return (batches // effective_accum) * self.trainer.max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc20a8-7787-4b1c-95da-53a090acb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWAResnet(LitResnet):\n",
    "    def __init__(self, trained_model, lr=0.01):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(\"lr\")\n",
    "        self.model = trained_model\n",
    "        self.swa_model = AveragedModel(self.model)\n",
    "        self.swa_start_step = 100\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.swa_model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        if self.trainer.global_step > self.swa_start_step:\n",
    "#             pdb.set_trace()\n",
    "            self.swa_model.update_parameters(self.model)\n",
    "            self.swa_scheduler.step()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "#     def training_epoch_end(self, training_step_outputs):\n",
    "#         self.swa_model.update_parameters(self.model)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, stage=None):\n",
    "        x, y = batch\n",
    "        logits = F.log_softmax(self.model(x), dim=1)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.hparams.lr, momentum=0.9, weight_decay=5e-4)\n",
    "        self.swa_scheduler = SWALR(optimizer, swa_lr=self.args.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def on_train_end(self):\n",
    "        update_bn(self.datamodule.train_dataloader(), self.swa_model, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f8c5fba-aa7c-4c65-8409-85df7a5a3410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f731b35-5a85-477e-82ad-6d2ce9d73086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateBNCallback(Callback):\n",
    "        \n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(\"Updating BatchNorm weights\")\n",
    "        torch.optim.swa_utils.update_bn(pl_module.train_dataloader(), pl_module.swa_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74a8a8dd-fd56-4679-89f8-ca69611e78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_monitor = LearningRateMonitor(logging_interval='step',log_momentum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa538d91-bc66-4175-9c39-73e06d0e8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"logs\", name=\"logging_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "647c71ad-a4c4-4caa-a078-ad8b5511b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple('Args',['lr','weight_decay','swa_start_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12529715-0842-4001-9732-f745b9d7917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(lr=0.01,weight_decay=1e-6,swa_start_step=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b6b6d58-f931-4f22-930c-dbd7459868fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BoringModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "319e561e-2624-4c86-a83e-77eb09da458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelswa = BoringModelSWA(args,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d99aae3e-9257-48c3-9f31-e5a60afc94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.DataLoader(RandomDataset(32, 64000), batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce14d3cc-7da3-4256-8883-3150443e1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "        accelerator='dp',\n",
    "        gpus=2,\n",
    "        limit_train_batches=700,\n",
    "        limit_val_batches=1,\n",
    "        num_sanity_val_steps=0,\n",
    "        max_epochs=1,\n",
    "        weights_summary=None,\n",
    "        callbacks=[UpdateBNCallback(), lr_monitor],\n",
    "        logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d21587e-b36c-460f-af0c-78a6d79432da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training steps : 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/jabbar/bin/anaconda3/envs/barlowbert/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/mounts/work/jabbar/bin/anaconda3/envs/barlowbert/lib/python3.8/site-packages/pytorch_lightning/callbacks/lr_monitor.py:112: RuntimeWarning: You are using `LearningRateMonitor` callback with models that have no learning rate schedulers. Please see documentation for `configure_optimizers` method.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57a3c09bfe549489ce4c29c7a7126c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating BatchNorm weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(modelswa, train_dataloaders=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ff087fc-f413-42e9-b366-630719b6d62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_monitor.lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ec3e541-2023-4f52-82bb-1fd9ffce2e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_monitor.last_momentum_values.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6994e6a9-75a7-436e-a878-8d4ded54c518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': -7.996602535247803, 'epoch': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logged_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7719c47-f0b1-483a-9424-35e7ce9c6f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
