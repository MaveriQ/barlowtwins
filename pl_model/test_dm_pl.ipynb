{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65bac59e-3375-4d9e-9768-441bde6928be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from barlowbert_dm import DataCollatorForBarlowBertWithMLM\n",
    "from barlowbert_models import BarlowBert\n",
    "from transformers import BertTokenizerFast, CONFIG_MAPPING, BertModel\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23fde9ee-f495-4c48-823a-0c62aebe4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', padding=True,truncation=True,)\n",
    "collator = DataCollatorForBarlowBertWithMLM(tokenizer,mlm_probability=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d3cd69-b445-4f11-a1ee-51da5f3ae326",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_disk('/mounts/data/proj/jabbar/barlowbert/bookcorpus_20mil_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f45cd8b9-9c7e-4013-bd34-5ab6ca48ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e8b356-e6c2-4e70-b137-6a4778f05cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(data, batch_size=4, num_workers=2,\n",
    "                                            pin_memory=True, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e64801f-8cc6-475f-b88e-cb05beedfa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y) = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad89e89-82b5-4ed5-ab2b-edfdefbefb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tiny = {\n",
    "    \"hidden_size\" : 128 ,\n",
    "    \"num_hidden_layers\" : 2,\n",
    "    \"num_attention_heads\": int(128/64),\n",
    "    \"intermediate_size\" : int(128*4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5344d7a0-9209-4e74-9758-c5736d3644dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CONFIG_MAPPING['bert']()\n",
    "config.update(bert_tiny)\n",
    "config.projector='128-128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00b45997-9fd4-4db9-88f1-51dec6d64aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_model = BarlowBert(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc595f07-13e7-4e36-8d29-872d6b1053a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/mounts/Users/cisintern/jabbar/git/barlowtwins/pl_model/barlowbert_models.py\u001b[0m(92)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     90 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     91 \u001b[0;31m        \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 92 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     93 \u001b[0;31m        outputs = self.bert(\n",
      "\u001b[0m\u001b[0;32m     94 \u001b[0;31m            \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2b7d33151389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbb_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mounts/work/jabbar/bin/anaconda3/envs/barlowbert/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/barlowtwins/pl_model/barlowbert_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         outputs = self.bert(\n\u001b[1;32m     94\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/barlowtwins/pl_model/barlowbert_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         outputs = self.bert(\n\u001b[1;32m     94\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mounts/work/jabbar/bin/anaconda3/envs/barlowbert/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mounts/work/jabbar/bin/anaconda3/envs/barlowbert/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = bb_model(**y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "23ff8baa-4dcb-4c04-9c3e-ecc98195ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling(torch.nn.Module):\n",
    "    \"\"\"Performs pooling (max or mean) on the token embeddings.\n",
    "    Using pooling, it generates from a variable sized sentence a fixed sized sentence embedding. This layer also allows to use the CLS token if it is returned by the underlying word embedding model.\n",
    "    You can concatenate multiple poolings together.\n",
    "    :param word_embedding_dimension: Dimensions for the word embeddings\n",
    "    :param pooling_mode: Can be a string: mean/max/cls. If set, overwrites the other pooling_mode_* settings\n",
    "    :param pooling_mode_cls_token: Use the first token (CLS token) as text representations\n",
    "    :param pooling_mode_max_tokens: Use max in each dimension over all tokens.\n",
    "    :param pooling_mode_mean_tokens: Perform mean-pooling\n",
    "    :param pooling_mode_mean_sqrt_len_tokens: Perform mean-pooling, but devide by sqrt(input_length).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 word_embedding_dimension: int,\n",
    "                 pooling_mode: str = None,\n",
    "                 pooling_mode_cls_token: bool = False,\n",
    "                 pooling_mode_max_tokens: bool = False,\n",
    "                 pooling_mode_mean_tokens: bool = True,\n",
    "                 pooling_mode_mean_sqrt_len_tokens: bool = False,\n",
    "                 ):\n",
    "        super(Pooling, self).__init__()\n",
    "\n",
    "        self.config_keys = ['word_embedding_dimension',  'pooling_mode_cls_token', 'pooling_mode_mean_tokens', 'pooling_mode_max_tokens', 'pooling_mode_mean_sqrt_len_tokens']\n",
    "\n",
    "        if pooling_mode is not None:        #Set pooling mode by string\n",
    "            pooling_mode = pooling_mode.lower()\n",
    "            assert pooling_mode in ['mean', 'max', 'cls']\n",
    "            pooling_mode_cls_token = (pooling_mode == 'cls')\n",
    "            pooling_mode_max_tokens = (pooling_mode == 'max')\n",
    "            pooling_mode_mean_tokens = (pooling_mode == 'mean')\n",
    "\n",
    "        self.word_embedding_dimension = word_embedding_dimension\n",
    "        self.pooling_mode_cls_token = pooling_mode_cls_token\n",
    "        self.pooling_mode_mean_tokens = pooling_mode_mean_tokens\n",
    "        self.pooling_mode_max_tokens = pooling_mode_max_tokens\n",
    "        self.pooling_mode_mean_sqrt_len_tokens = pooling_mode_mean_sqrt_len_tokens\n",
    "\n",
    "        pooling_mode_multiplier = sum([pooling_mode_cls_token, pooling_mode_max_tokens, pooling_mode_mean_tokens, pooling_mode_mean_sqrt_len_tokens])\n",
    "        self.pooling_output_dimension = (pooling_mode_multiplier * word_embedding_dimension)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Pooling({})\".format(self.get_config_dict())\n",
    "\n",
    "    def get_pooling_mode_str(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the pooling mode as string\n",
    "        \"\"\"\n",
    "        modes = []\n",
    "        if self.pooling_mode_cls_token:\n",
    "            modes.append('cls')\n",
    "        if self.pooling_mode_mean_tokens:\n",
    "            modes.append('mean')\n",
    "        if self.pooling_mode_max_tokens:\n",
    "            modes.append('max')\n",
    "        if self.pooling_mode_mean_sqrt_len_tokens:\n",
    "            modes.append('mean_sqrt_len_tokens')\n",
    "\n",
    "        return \"+\".join(modes)\n",
    "\n",
    "    def forward(self, bert_encoder_output,attention_mask):\n",
    "        pdb.set_trace()\n",
    "        token_embeddings = bert_encoder_output.last_hidden_state\n",
    "        cls_token = bert_encoder_output.last_hidden_state[:,0]\n",
    "\n",
    "        ## Pooling strategy\n",
    "        output_vectors = []\n",
    "        if self.pooling_mode_cls_token:\n",
    "            output_vectors.append(cls_token)\n",
    "        if self.pooling_mode_max_tokens:\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
    "            max_over_time = torch.max(token_embeddings, 1)[0]\n",
    "            output_vectors.append(max_over_time)\n",
    "        if self.pooling_mode_mean_tokens or self.pooling_mode_mean_sqrt_len_tokens:\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "\n",
    "            #If tokens are weighted (by WordWeights layer), feature 'token_weights_sum' will be present\n",
    "#             if 'token_weights_sum' in features:\n",
    "#                 sum_mask = features['token_weights_sum'].unsqueeze(-1).expand(sum_embeddings.size())\n",
    "#             else:\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "            if self.pooling_mode_mean_tokens:\n",
    "                output_vectors.append(sum_embeddings / sum_mask)\n",
    "            if self.pooling_mode_mean_sqrt_len_tokens:\n",
    "                output_vectors.append(sum_embeddings / torch.sqrt(sum_mask))\n",
    "\n",
    "        output_vector = torch.cat(output_vectors, 1)\n",
    "#         features.update({'sentence_embedding': output_vector})\n",
    "        return output_vector\n",
    "\n",
    "    def get_sentence_embedding_dimension(self):\n",
    "        return self.pooling_output_dimension\n",
    "\n",
    "    def get_config_dict(self):\n",
    "        return {key: self.__dict__[key] for key in self.config_keys}\n",
    "\n",
    "    def save(self, output_path):\n",
    "        with open(os.path.join(output_path, 'config.json'), 'w') as fOut:\n",
    "            json.dump(self.get_config_dict(), fOut, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(input_path):\n",
    "        with open(os.path.join(input_path, 'config.json')) as fIn:\n",
    "            config = json.load(fIn)\n",
    "\n",
    "        return Pooling(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dc3bcac8-493e-4f69-b3d8-583f0b4361f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.mean_pooling = False\n",
    "config.max_pooling = False\n",
    "config.cls_pooling = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "150863db-f35f-4f35-b9dc-cdb3d0ce5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.output_attentions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "851c076d-3faa-4d6c-a7ec-d0785600cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooler = Pooling(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "76e7adb9-d44b-41cf-a479-6e26dc8d4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0d91983a-8fd9-4eed-904a-e965bbbb77d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = model(**y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d63aa123-e740-4358-a593-619d0606588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-81-854b509078dd>\u001b[0m(62)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     60 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_encoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     61 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 62 \u001b[0;31m        \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_encoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     63 \u001b[0;31m        \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     64 \u001b[0;31m        \u001b[0mcls_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_encoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    }
   ],
   "source": [
    "pool = pooler(enc,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "717c985d-a01a-4900-9370-0b96bf0d2bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5efe871d-5207-4fc4-b770-515aeae2a451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"cls_pooling\": true,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_pooling\": false,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"mean_pooling\": false,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"output_attentions\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"projector\": \"128-128\",\n",
       "  \"transformers_version\": \"4.8.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbc456a6-e5bf-4c27-8e4e-ca5c24b7cb42",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'expand_like'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-5a49669161f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'expand_like'"
     ]
    }
   ],
   "source": [
    "enc.attentions[-1].unsqueeze(-1).expand_like(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248bdb4a-12e7-4fe8-a037-fb3cf618e0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
