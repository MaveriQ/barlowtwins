{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49f3c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerBase, BertTokenizer, BatchEncoding, BertModel, BertConfig\n",
    "from copy import copy\n",
    "import pdb\n",
    "from datasets import load_dataset\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82e6ddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset bookcorpus (/mounts/data/corp/huggingface/datasets/bookcorpus/plain_text/1.0.0/44662c4a114441c35200992bea923b170e6f13f2f0beb7c14e43759cec498700)\n"
     ]
    }
   ],
   "source": [
    "corpus = load_dataset('bookcorpus',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37e6855e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the half-ling book one in the fall of igneeria series kaylee soderburg copyright 2013 kaylee soderburg all rights reserved .'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7d03c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple)):\n",
    "        examples = [torch.tensor(e, dtype=torch.long) for e in examples]\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "    length_of_first = examples[0].size(0)\n",
    "    are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return torch.stack(examples, dim=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(x.size(0) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n",
    "    for i, example in enumerate(examples):\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            result[i, : example.shape[0]] = example\n",
    "        else:\n",
    "            result[i, -example.shape[0] :] = example\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4c74411",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForLanguageModeling:\n",
    "    \"\"\"\n",
    "    Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they\n",
    "    are not all of the same length.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n",
    "            The tokenizer used for encoding the data.\n",
    "        mlm (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "            Whether or not to use masked language modeling. If set to :obj:`False`, the labels are the same as the\n",
    "            inputs with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for\n",
    "            non-masked tokens and the value to predict for the masked token.\n",
    "        mlm_probability (:obj:`float`, `optional`, defaults to 0.15):\n",
    "            The probability with which to (randomly) mask tokens in the input, when :obj:`mlm` is set to :obj:`True`.\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For best performance, this data collator should be used with a dataset having items that are dictionaries or\n",
    "        BatchEncoding, with the :obj:`\"special_tokens_mask\"` key, as returned by a\n",
    "        :class:`~transformers.PreTrainedTokenizer` or a :class:`~transformers.PreTrainedTokenizerFast` with the\n",
    "        argument :obj:`return_special_tokens_mask=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm: bool = True\n",
    "    mlm_probability: float = 0.15\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.mlm and self.tokenizer.mask_token is None:\n",
    "            raise ValueError(\n",
    "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. \"\n",
    "                \"You should pass `mlm=False` to train on causal language modeling instead.\"\n",
    "            )\n",
    "\n",
    "    def __call__(\n",
    "        self, examples: List[Union[List[int], torch.Tensor, Dict[str, torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], (dict, BatchEncoding)):\n",
    "#             pdb.set_trace()\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\"input_ids\": _collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)}\n",
    "#         pdb.set_trace()\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        batch_1 = copy(batch)\n",
    "        batches=[]\n",
    "        \n",
    "\n",
    "        batch[\"input_ids\"], masked_indices_1 = self.mask_tokens(batch[\"input_ids\"], special_tokens_mask=special_tokens_mask)\n",
    "        batches.append(batch)\n",
    "        batch_1[\"input_ids\"], masked_indices_2 = self.mask_tokens(batch_1[\"input_ids\"], special_tokens_mask=special_tokens_mask)\n",
    "        batches.append(batch_1)\n",
    "        \n",
    "        batches.append({'masked_indices': masked_indices_1 | masked_indices_2})\n",
    "            \n",
    "        return batches\n",
    "\n",
    "    def mask_tokens(\n",
    "        self, inputs: torch.Tensor, special_tokens_mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "#         labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "#         pdb.set_trace()\n",
    "        return inputs, masked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b16fcd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2c9b3b5-3806-465b-915c-c5fda3c7dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = BertWordPieceTokenizer('bert-base-uncased-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ca25912",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForLanguageModeling(tokenizer,mlm_probability=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b7995b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [tokenizer(corpus[i]['text'],return_tensors=\"pt\",padding='max_length',max_length=32,return_special_tokens_mask=True,return_token_type_ids=False) for i in range(4) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16385387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.encode(\"The capital of France is Paris.\",add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cffc6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\",padding='max_length',max_length=16,return_special_tokens_mask=True,return_token_type_ids=False)\n",
    "input_2 = tokenizer(\"I am going to Berlin.\", return_tensors=\"pt\",padding='max_length',max_length=16,return_special_tokens_mask=True,return_token_type_ids=False)\n",
    "input_3 = tokenizer(\"It's a beautiful day today\", return_tensors=\"pt\",padding='max_length',max_length=16,return_special_tokens_mask=True,return_token_type_ids=False)\n",
    "input_4 = tokenizer(\"Where are you going so early in the morning?\", return_tensors=\"pt\",padding='max_length',max_length=16,return_special_tokens_mask=True,return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d69afccc-031e-4ac5-a331-da838b6a7ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[ 101, 1996, 3007, 1997, 2605, 2003, 3000, 1012,  102,    0,    0,\n",
       "             0,    0,    0,    0,    0]],\n",
       "\n",
       "        [[ 101, 1045, 2572, 2183, 2000, 4068, 1012,  102,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0]],\n",
       "\n",
       "        [[ 101, 2009, 1005, 1055, 1037, 3376, 2154, 2651,  102,    0,    0,\n",
       "             0,    0,    0,    0,    0]]]), 'special_tokens_mask': tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad([input_1,input_2,input_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ba592cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_1 = tokenizer(corpus[0]['text'], return_tensors=\"pt\",padding='max_length',max_length=16,return_special_tokens_mask=True,return_token_type_ids=False)\n",
    "# input_2 = tokenizer(corpus[1]['text'], return_tensors=\"pt\",padding='max_length',max_length=16,return_special_tokens_mask=True,return_token_type_ids=False)\n",
    "# input_3 = tokenizer(corpus[2]['text'], return_tensors=\"pt\",padding='max_length',max_length=16,return_special_tokens_mask=True,return_token_type_ids=False)\n",
    "# input_4 = tokenizer(corpus[3]['text'], return_tensors=\"pt\",padding='max_length',max_length=16,return_special_tokens_mask=True,return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d764758b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1996, 3007, 1997, 2605, 2003, 3000, 1012,  102,    0,    0,    0,\n",
       "            0,    0,    0,    0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a87f3332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[ 101, 1996, 3007, 1997, 2605, 2003, 3000, 1012,  102,    0,    0,    0,\n",
       "             0,    0,    0,    0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[ 101, 1045, 2572, 2183, 2000, 4068, 1012,  102,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[input_1,input_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c453419",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_collated = collator([input_1,input_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bd495a6-e8ac-4f13-8068-bc67cc846b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[[  101,   103, 16156,   103,   103,   103,   103,   103,   102,     0,\n",
       "               0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "         [[  101,   103,    50,   103,   103,   103,   103,   102,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]])},\n",
       " {'input_ids': tensor([[[  101,   103, 16156,   103,   103,   103,   103,   103,   102,     0,\n",
       "               0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "         [[  101,   103,    50,   103,   103,   103,   103,   102,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]])},\n",
       " {'masked_indices': tensor([[[False,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "            False, False, False, False, False, False]],\n",
       "  \n",
       "          [[False,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "            False, False, False, False, False, False]]])}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bdce000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_collated[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "425e27fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[101, 103, 103, 103, 103, 103, 103, 103, 102,   0,   0,   0,   0,   0,\n",
       "            0,   0]],\n",
       "\n",
       "        [[101, 103, 103, 103, 103, 103, 103, 102,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_collated[1]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e1f28cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[101, 103, 103, 103, 103, 103, 103, 103, 102,   0,   0,   0,   0,   0,\n",
       "            0,   0]],\n",
       "\n",
       "        [[101, 103, 103, 103, 103, 103, 103, 102,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_collated[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d24a4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "          False, False, False, False, False, False]],\n",
       "\n",
       "        [[False,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "          False, False, False, False, False, False]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_collated[2]['masked_indices']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8fa47",
   "metadata": {},
   "source": [
    "## BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0fe22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d530b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tiny = {\n",
    "    \"hidden_size\" : 128 ,\n",
    "    \"num_hidden_layers\" : 2,\n",
    "    \"num_attention_heads\": int(128/64),\n",
    "    \"intermediate_size\" : int(128*4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(bert_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04686168",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel(config, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39292daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = inp_collated[0]\n",
    "out1 = bert_model(input_ids = inp['input_ids'].squeeze(),\n",
    "                  attention_mask = inp['attention_mask'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59fff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = inp_collated[1]\n",
    "out2 = bert_model(input_ids = inp['input_ids'].squeeze(),\n",
    "                  attention_mask = inp['attention_mask'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56cf792",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_indices = inp_collated[2]['masked_indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e03e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = out1.last_hidden_state.view(-1,128)[masked_indices.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "second = out2.last_hidden_state.view(-1,128)[masked_indices.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(first.T@second).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5fcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7bf96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ebfd7dcb4380eb144eef06d93b174c153b1a942e887284da88a2b1dfce04ba76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
